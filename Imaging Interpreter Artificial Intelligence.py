import torch
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader, Subset, random_split
import random

seed = 27
torch.manual_seed(seed) # commands that the random numbers generated by PyTorch are consistent when running the code different times in order to guarantee reproducibility.
np.random.seed(seed) # does the same command as the previous code line but is applied to every case NumPy is used.
random.seed(seed) # does the same as the previous commands but for the random module
device = torch.device("cpu") # Commands were the PyTorch operations will be performed: the cpu.

# CNN Architecture

class SimpleCNN(nn.Module): # SimpleCNN inherits from the parent class nn.Module so that it can use its relevant methods
    def __init__(self): # self refers to the instance of the call for example "a" is an instance of the class if a = SimpleCNN()
        super(SimpleCNN, self).__init__() #super(SimpleCNN, self).__init__() establishes the order in which Python will process the call until finding the __init__() method inside of the parent class nn.Module
        self.conv1 = nn.Conv2d(1, 16, 3) # 1 input channel (because we are in grayscale just one color channel), 16 output channels, and kernel size = 3. There would be 16 kernels each with one 3x3 matrix and a bias term
        self.batch_norm1 = nn.BatchNorm2d(16) # Batch normalization is performed
        self.pool = nn.MaxPool2d(2, 1) # Max-pooling layer with kernel size of 2 and stride (how long is the horizontal and vertical step when overlapping matrices) of 1
        self.conv2 = nn.Conv2d(16, 16, 3) # Second Convolutional Layer with 16 input channels (from previous output channels max-pooled), 16 output channels, and kernel size = 3
        self.batch_norm2 = nn.BatchNorm2d(16) # Batch normalization is performed again
        self.conv3 = nn.Conv2d(16, 16, 3) # Third Convolutional Layer with the same architecture as the previous layer
        self.average_pooling = nn.AvgPool2d(5,5) # Average pooling is performed considering 5x5 sub-matrices and a horizontal and vertical step of 5 entries
        self.batch_norm3= nn.BatchNorm2d(16) # Batch normalization
        self.fc1 = None # First Fully Connected Layer later updated because the data has to get flattened to pass to the feed-forward neural network stage of the CNN
        self.fc2 = nn.Linear(120, 84) # Second Fully Connected Layer with 120 input neurons and 84 output neurons
        self.fc3 = nn.Linear(84, 4) # Third Fully Connected Layer with 84 input neurons and 4 output neurons (corresponding the number of classes)

    def forward(self, x):
        x = self.pool(self.batch_norm1(F.relu(self.conv1(x)))) # Composed function, first conv1 is applied, then ReLU, and ultimately max pooling # F.relu is a function from the torch.nn.functional module
        x = self.pool(self.batch_norm2(F.relu(self.conv2(x)))) # Composed function based from the previous output, first conv2 is applied, then ReLU, and ultimately max pooling
        x = self.average_pooling(self.pool(self.batch_norm3(F.relu(self.conv3(x))))) # The same steps with the distinction of additionally applying average pooling which is defined earlier

        quantification_of_information = x.size(1) * x.size (2) * x.size(3)

        x = x.view(x.size(0), quantification_of_information) # Flattens the matrix from the convolutional stage such that each row represents a sample in the batch and each column information extracted from the neural network
        
        if self.fc1 is None:
            self.fc1 = nn.Linear(quantification_of_information, 120) # first argument represents number of input neurons and the second number of output neurons

        x = F.relu(self.fc1(x)) # Fully connected stage, composed function first with general structure MX + B, then ReLU
        x = F.relu(self.fc2(x)) # Composed function first with general structure again based on the input of the previous layer, then ReLU
        x = self.fc3(x) # Finally general structure applied again, where x becomes the final output of the network
        return x 
    
class NumpyDataset(Dataset):
    def __init__(self, images_path, labels_path, transform = None):
        self.images = np.load(images_path) # Loads the numpy array in images_path
        self.labels = np.load(labels_path) # Loads the numpy array in labels_path
        self.transform = transform # self.transform = None unless otherwise specified in the third argument when creating an instance serves for data processing

    def __len__(self): # When len() is called for a instance of a class, Python will look for the method __len__(self) and then determine the length. If it weren't specified that would generate an error
        return len(self.images) # Let's us know the number of samples (images)
    
    def __getitem__(self, idx): #__getitem__ makes it possible that instances of a class can be called using sub notation such as x[index] # idx = index
        image = self.images[idx] # gets the component of the array self.images based on the idx provided
        label = self.labels[idx] # gets the component of the array self.labels based on the idx provided

        if self.transform != None:
            image = self.transform(image) # self.transform(image) = transform(image)

        label = np.argmax(label) # Extracts the index of the maximum value in a label. Since, we have one-hot encoding i.e. [0,0,1,0] to represent that the label is the third one, this function  transforms the one-hot encoding to the integer 2 (because of Python's enumeration system)
        
        label = torch.tensor(label, dtype = torch.long) # Converts the integer label into a PyTorch tensor # dtype = torch.long = 64-bit integer, which means that to represent an integer 64 bits of memory are used and the possible integers that could be turned into a dtype = torch.long are within the closed interval [-2^63, 2^63]- Any value outside would cause an error. That's why a high exponent is considered, to make the range considered as big a possible. Now it is not a problem because we have just integers from 0 to 4 but if some clinician wants to add more classes, a higher and higher index is necessary. Especially when working with an inmense number of possible classifications.
        
        return image, label # an specific sample and its corresponding integer (label) as a tensor

def main(): # Indicates the main processes that will be initialized if the code runs in the IDE or in the terminal
    transform = transforms.Compose([ # transforms.Compose applies the transformations within it to an image in sequence.
        transforms.ToTensor(), # converts the NumPy array into a PyTorch tensor
        transforms.Normalize((0.5,),(0.5,)) # the training, testing, and validation values are normalized with 0.5 mean and standard deviation.
    ])

    train_dataset = NumpyDataset("training_images.npy", "training_labels.npy", transform = transform) # The described transformation is applied
    test_dataset = NumpyDataset("testing_images.npy", "testing_labels.npy", transform = transform)
    
    print(f"Number of images in the training dataset: {len(train_dataset)}")

    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle = True, num_workers = 0) # trainloader contains all the batches of size 16 for the training dataset, batch 1, batch 2, so on

    test_size = len(test_dataset) // 2 # does floor division: gives back the largest or equal integer to the result
    val_size = len(test_dataset) - test_size
    val_dataset, test_dataset = random_split(test_dataset, [val_size, test_size])

    print(f"Number of images in the validation dataset: {len(val_dataset)}")
    print(f"Number of images in the testing dataset: {len(test_dataset)}")
    print(f"Percentages of data with respect to the total amount of images.\nTraining Dataset: {100*(len(train_dataset)/(len(train_dataset) + len(val_dataset) + len(test_dataset))):.2f}%\nValidation Dataset: {100*(len(val_dataset)/(len(train_dataset) + len(val_dataset) + len(test_dataset))):.2f}%\nTesting Dataset: {100*(len(test_dataset)/(len(train_dataset) + len(val_dataset) + len(test_dataset))):.2f}%")

    valloader = DataLoader(val_dataset, batch_size = 16, shuffle = True, num_workers = 0)
    testloader = torch.utils.data.DataLoader(test_dataset, batch_size = 16, shuffle = True, num_workers = 0)

    net = SimpleCNN() # An instance for the class SimpleCNN is created
    torch.manual_seed(seed)
    criterion = nn.CrossEntropyLoss() # Loss function
    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9) # Momentum explained in the paper, net.parameters() is a way to access all learnable parameters in the network, so they are optimized based on the process mentioned in backpropagation

    # Trackers of Accuracies, Standard Errors, and Last Mini-Batch Accuracies
    accuracies = []
    ci_margins_95 = []
    epochs = []
    last_mini_batch_accuracies = []

    for epoch in range(25):
        print(f'Starting epoch {epoch + 1}')

        torch.manual_seed(seed + epoch)
        np.random.seed(seed + epoch)
        random.seed(seed + epoch) # reiterations of these commands ensures reproducibility

        running_loss = 0.0
        correct = 0
        total = 0
        last_mini_batch_accuracy = 0.0
        for i, data in enumerate(trainloader, 0): # 0 is the starting index # i will be 0,1,2,... accordingly and data will be a tuple containg two components each time: the input image data, and its corresponding label.
            inputs, labels = data # tuple unpacking: inputs = data[0], labels = data[1]

            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad() # make the gradients equal to 0 because otherwise they will be added together

            outputs = net(inputs) # PyTorch nn.Module calls the forward method inside the SimpleCNN automatically class and passes inputs as x so get a returned x that is the output of the neural network
            loss = criterion(outputs, labels) # Applies CrossEntropyLoss with respect to the neural network output and the ground-truth labels
            loss.backward() # partial of L / partial of y, and computes the gradients of the loss per each layer's output, and then computes the gradients w.r.t all the learnable parameters for all the layers via backpropagation. All the gradients are stored. Can access to them via param.grad. 
            optimizer.step() # updates the model parameters

            # Comments of progress in training
            running_loss += loss.item() # adds the mini-batch loss of the neural network
            _,predicted = torch.max(outputs.data, 1) # finds the maximum values per row in outputs.data which is a matrix were each column represents how confident the network is about a sample being a specific label. If in a row, the third column is the one with the highest number that represents that the network is most confident that its sample should be classified as the corresponding label of column three.
            total += labels.size(0) # calculates the number of samples
            correct += (predicted == labels).sum().item() # counts the number of correct predictions
            mini_batch_loss = loss.item()
            mini_batch_accuracy = 100 * correct / total
            
            if i == len(trainloader) - 1:
                last_mini_batch_accuracy = mini_batch_accuracy

            print(f"[Epoch {epoch + 1}, Mini-batch {i + 1}] "
                  f"Loss: {mini_batch_loss:.4f}, "
                  f"Accuracy: {mini_batch_accuracy:.2f}%")
        print(f"Epoch {epoch + 1} finished.")

        correct_val = 0
        total_val = 0
        all_labels_val = []
        all_preds_val = []

        with torch.no_grad():
            for data in valloader:
                images, labels = data
                outputs = net(images)
                _, predicted = torch.max(outputs.data, 1)
                total_val += labels.size(0)
                correct_val += (predicted == labels).sum().item()

                all_labels_val.extend(labels.cpu().numpy())
                all_preds_val.extend(predicted.cpu().numpy())

        accuracy_val = correct_val / total_val
        ci_margin_95 = 1.96 * (accuracy_val*(1-accuracy_val) / total_val) ** 0.5 

        last_epoch_val_accuracy = accuracy_val
        accuracies.append(accuracy_val)
        ci_margins_95.append(ci_margin_95)
        last_mini_batch_accuracies.append(last_mini_batch_accuracy)
        epochs.append(epoch + 1)

        print(f"Validation Dataset Accuracy for Epoch {epoch + 1}: {100 * accuracy_val:.2f}%,\n 95% Confidence Interval Margin: {ci_margin_95:.4f}") # n is the number of images in the validation dataset

        correct_test = 0
        total_test = 0
        all_labels_test = []
        all_preds_test = []

        with torch.no_grad():
            for data in testloader:
                images, labels = data
                outputs = net(images)
                _, predicted = torch.max(outputs.data, 1)
                total_test += labels.size(0)
                correct_test += (predicted == labels).sum().item()
                all_labels_test.extend(labels.cpu().numpy())
                all_preds_test.extend(predicted.cpu().numpy())
        
        last_epoch_test_accuracy = correct_test / total_test
    print("Finished Training")

    # Graph for epoch decision making

    plt.figure(figsize = (10, 6))
    plt.plot(epochs, [100 * acc for acc in accuracies], marker = "o", label = "Validation Accuracy")
    plt.scatter(epochs, last_mini_batch_accuracies, color = "green", label = "Last Mini-Batch Accuracy")

    best_epoch = np.argmax(accuracies)
    best_accuracy = accuracies[best_epoch]
    best_ci_margin_95 = ci_margins_95[best_epoch]

    plt.errorbar(epochs[best_epoch], 100 * best_accuracy, yerr = 100 * best_ci_margin_95, fmt = "o", color = "red", ecolor = "red", capsize = 5, label = "Best Epoch with 95% CI Margin")
    plt.axhline(y=100 * (best_accuracy + best_ci_margin_95), color = "red", linestyle = "--")
    plt.axhline(y=100*(best_accuracy - best_ci_margin_95), color = "red", linestyle = "--")

    plt.xlabel("Epoch")
    plt.ylabel("Validation Accuracy (%)")
    plt.title("Validation and Last Mini-Batch Accuracy per Epoch")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Network Final Results

    print(f"Accuracy of the network on the validation dataset images: {100 * last_epoch_val_accuracy:.2f}%")

    cm_val = confusion_matrix(all_labels_val, all_preds_val)

    plt.figure(figsize = (8, 6))
    sns.heatmap(cm_val, annot = True, fmt = "d", cmap = "Blues")
    plt.title("Confusion Matrix - Validation Dataset")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.show()

    print(f'Accuracy of the network on the testing dataset images: {100 * last_epoch_test_accuracy:.2f}%')

    cm_test = confusion_matrix(all_labels_test, all_preds_test)

    plt.figure(figsize = (8, 6))
    sns.heatmap(cm_test, annot=True, fmt="d", cmap="Blues")
    plt.title("Confusion Matrix - Testing Dataset")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.show()

if __name__ == "__main__": # Once the code is initialized either in the IDE or in the terminal, __name__ turns to "__main__" and that initializes main()
    main()